# Web Scraping {#web-mining}

Gonzalo Barr√≠a^[E-mail: ghbarria@uc.cl]

### Lecturas sugeridas {-}

- Calvo, E. (2015). *Anatom√≠a pol√≠tica de Twitter en Argentina. Tuiteando #Nisman*. Buenos Aires: Capital Intelectual.

- Steinert-Threlkeld, Z. (2018). *Twitter as Data (Elements in Quantitative and Computational Methods for the Social Sciences)*. Cambridge: Cambridge University Press.

### Los paquetes que necesitas instalar {-}

- `tidyverse` [@R-tidyverse], `glue` [@R-glue], `rvest` [@R-rvest], `rtweet` [@R-rtweet].

## Introducci√≥n

La informaci√≥n en Internet crece exponencialmente cada d√≠a. Si el problema de la ciencia pol√≠tica en el siglo XX fue la falta de datos para probar hip√≥tesis, el siglo XXI presenta otro desaf√≠o: la informaci√≥n es abundante y est√° al alcance de la mano, pero hay que saber c√≥mo recopilarla y analizarla. Esta enorme cantidad de datos est√° generalmente disponible pero de forma no estructurada, por lo que cuando te encuentres con la informaci√≥n tendr√°s dos retos principales: extraerla y luego clasificarla (es decir, dejarla como *datos organizados (tidy)*).

Una de las t√©cnicas m√°s utilizadas para extraer informaci√≥n de los sitios web es la t√©cnica conocida como *web scraping*.



El web scraping se est√° convirtiendo en una t√©cnica cada vez m√°s popular en el an√°lisis de datos debido a su versatilidad para tratar con diferentes sitios web. Como podemos ver en el siguiente gr√°fico, las b√∫squedas en Google del t√©rmino "web scraping" han crecido constantemente a√±o tras a√±o desde 200:

<div class="figure" style="text-align: center">
<img src="12-web-mining_es_files/figure-html/unnamed-chunk-2-1.png" alt="B√∫squedas de 'web scraping' en Google" width="672" />
<p class="caption">(\#fig:unnamed-chunk-2)B√∫squedas de 'web scraping' en Google</p>
</div>

Esto consiste en obtener datos no muy estructurados (HTML) desde un sitio web, que usualmente luego transformamos a un formato estructurado con filas y columnas, con el que es m√°s f√°cil trabajar. Nos permite obtener datos de fuentes no tradicionales (¬°pr√°cticamente cualquier p√°gina web!)

- La informaci√≥n que podemos obtener es la misma que podr√≠amos hacer manualmente (copiar y pegar a un documento de Excel, por ejemplo), pero podemos automatizar tareas muy tediosas.

¬øPara qu√© podr√≠amos usar el web scraping? Como ejemplo pr√°ctico, vamos a realizar 2 simples ejercicios de extracci√≥n de datos. Pero antes de eso es bueno estar familiarizado con las fuentes de informaci√≥n y los m√∫ltiples usos que se le pueden dar a los datos extra√≠dos. Algunas otras posibles aplicaciones para las que se puede usar el web scraping son:

1. Por ejemplo, pueden utilizarse para clasificar productos o servicios para crear motores de recomendaci√≥n, para obtener datos de texto, como en la Wikipedia, para hacer sistemas basados en el Procesamiento del Lenguaje Natural. 

2. Generar datos a partir de etiquetas de im√°genes, de sitios web como Google, Flickr, etc. para entrenar modelos de clasificaci√≥n de im√°genes.

3. Consolidar los datos de las redes sociales: Facebook y Twitter, para realizar an√°lisis de sentimientos u opiniones.

4. Extraer comentarios de usuarios y sitios de comercio electr√≥nico como Alibaba, Amazon o Walmart.

## Formas de hacer web scraping

Podemos raspar (scrape) los datos, es decir, obtenerlos, de diferentes maneras:

1. Copiar y pegar: obviamente esto debe ser hecho por un humano, es una manera lenta e ineficiente de obtener datos de la web.
   
2. Uso de APIs: API (por sus siglas en ingl√©s) significa *interfaz de programaci√≥n de aplicaciones*. Sitios web como Facebook, Twitter, LinkedIn, entre otros, ofrecen una API p√∫blica y/o privada, a la que se puede acceder mediante programaci√≥n, para recuperar datos en el formato deseado. Una API es un conjunto de definiciones y protocolos utilizados para desarrollar e integrar programas inform√°ticos de aplicaci√≥n. En pocas palabras, es un c√≥digo que indica a las aplicaciones c√≥mo pueden comunicarse entre s√≠. Las API permiten que sus productos y servicios se comuniquen con otros, sin necesidad de saber c√≥mo se implementan. Esto simplifica el desarrollo de aplicaciones y ahorra tiempo y dinero.

3. An√°lisis DOM: DOM significa *Documento Objeto Modelo* y es esencialmente una interfaz de plataforma que proporciona un conjunto est√°ndar de objetos para representar documentos HTML y XML. El DOM permite el acceso din√°mico a trav√©s de la programaci√≥n para acceder, a√±adir y cambiar din√°micamente el contenido estructurado en documentos con lenguajes como *JavaScript*. A trav√©s de algunos programas es posible recuperar el contenido din√°mico, o partes de sitios web generados por scripts de un cliente. Los objetos DOM modelan tanto la ventana del navegador como el historial, el documento o la p√°gina web, y todos los elementos que la propia p√°gina puede tener, como p√°rrafos, divisiones, tablas, formularios y sus campos, etc. A trav√©s del DOM se puede acceder, mediante Javascript, a cualquiera de estos elementos, es decir, a sus correspondientes objetos para alterar sus propiedades o invocar sus m√©todos. Sin embargo, a trav√©s del DOM, cualquier elemento de la p√°gina est√° disponible para los programadores de Javascript, para modificarlos, borrarlos, crear nuevos elementos y colocarlos en la p√°gina. 



### Est√°ndar de exclusi√≥n de robot

Antes de entrar en la pr√°ctica del *web scraping* tenemos que entender mejor qu√© es y c√≥mo funciona el archivo *robots.txt* presente en la mayor√≠a de los sitios web. Este archivo contiene el llamado *est√°ndar de exclusi√≥n de robot* que es una serie de instrucciones especialmente dirigidas a los programas que buscan indexar el contenido de estas p√°ginas (por ejemplo el bot de Google que "guarda" las nuevas p√°ginas que se crean en Internet). Este m√©todo se utiliza para evitar que ciertos bots que analizan sitios de Internet a√±adan informaci√≥n "innecesaria" a los resultados de las b√∫squedas. Un archivo *robots.txt* en una p√°gina web funcionar√° como una petici√≥n para que ciertos bots ignoren archivos o directorios espec√≠ficos en su b√∫squeda. Esto es importante cuando hablamos de *web scraping* ya que siempre es aconsejable revisar el archivo *robots.txt* de una p√°gina web antes de iniciar el scraping ya que puede incluir informaci√≥n que necesitaremos m√°s adelante. Un ejemplo de c√≥mo es uno de estos archivos se puede encontrar en el archivo *robots.txt* de Google https://www.google.com/robots.txt

<div class="figure" style="text-align: center">
<img src="00-images/web-mining/web-mining4.png" alt="Vistazo a los est√°ndares de exclusi√≥n de Google" width="70%" />
<p class="caption">(\#fig:unnamed-chunk-3)Vistazo a los est√°ndares de exclusi√≥n de Google</p>
</div>

En esta imagen encontramos la expresi√≥n *Usuario-agente: *. Esto permite a todos los robots acceder a los archivos que se almacenan en el c√≥digo principal de la p√°gina web ya que el comod√≠n (*) significa "TODO". 
A continuaci√≥n vemos que los robots no pueden indexar o visitar p√°ginas web del tipo */index.html?* o */grupos* por ejemplo. Si encontramos la expresi√≥n *Rechazar: /* significa que se le niega el acceso a todos los bots (el comod√≠n */* se aplica autom√°ticamente a todos los archivos almacenados en el directorio ra√≠z del sitio web).


## Web scraping en R

Iremos cargando los paquetes que necesitamos. Ya est√°s familiarizado con algunos de ellos como `ggplot2`, que es parte del `tidyverse`.  El paquete `rvest` es el que nos permitir√° hacer el scraping de datos con R. Es esencialmente una librer√≠a que nos permite traer y manipular datos de una p√°gina web, usando HTML y XML. Finalmente, el paquete `glue` est√° dise√±ado para hacer m√°s f√°cil interpolar (pegar) datos en *strings¬®*


```r
library(tidyverse)
library(glue)
library(rvest)
```

Para "leer" datos de diferentes sitios web necesitaremos la ayuda de una herramienta de c√≥digo abierto (open source tool), un plugin llamado "selectorgadget". Este se utiliza para extraer informaci√≥n de un sitio web. En este caso lo usaremos para seleccionar y resaltar las partes del sitio web que queremos extraer.

Se puede encontrar m√°s informaci√≥n en este enlace https://selectorgadget.com/ y aqu√≠ https://cran.r-project.org/web/packages/rvest/vignettes/selectorgadget.html

### Ejemplo aplicado: los comunicados de prensa de la Organizaci√≥n de Estados Americanos (OEA)

Como primer ejemplo haremos un web scraping de un sitio est√°tico. Es decir, un sitio web que tiene texto en HTML y que no cambia. Supongamos que estamos trabajando en un proyecto de diplomacia y relaciones internacionales en el que debemos sistematizar la informaci√≥n sobre la interacci√≥n entre los pa√≠ses de Am√©rica Latina. Uno de los repositorios m√°s √∫tiles para iniciar este tipo de investigaci√≥n es el sitio web de la Organizaci√≥n de Estados Americanos (OEA). Se puede encontrar en el siguiente link: https://www.oas.org/es/. 

Este sitio ofrece informaci√≥n muy pertinente para los analistas de datos pol√≠ticos, ya que se trata de datos no estructurados que permiten identificar, por ejemplo, las redes de pa√≠ses, las alianzas, las fuentes de conflicto y las cuestiones que los ministerios de relaciones exteriores consideran pertinentes.

Antes de comenzar con el *web scraping* en s√≠ mismo analizaremos el archivo *robots-txt* del sitio web de la OEA http://oas.org/robots.txt

<div class="figure" style="text-align: center">
<img src="00-images/web-mining/web-mining3.png" alt="Vistazo al archivo robots.txt de la OEA" width="100%" />
<p class="caption">(\#fig:unnamed-chunk-5)Vistazo al archivo robots.txt de la OEA</p>
</div>

Encontramos que algunos directorios est√°n prohibidos de ser indexados, pero todos los bots y usuarios est√°n autorizados a visitar el sitio. Lo m√°s importante es la expresi√≥n *Crawl-delay: 3* que b√°sicamente nos dice que por cada petici√≥n hecha por un robot es aconsejable esperar 3 segundos entre una consulta y otra para no saturar el sitio, lo que puede resultar en que te bloquee.

En este ejemplo en particular, queremos extraer los t√≠tulos de los comunicados de prensa que se encuentran en este sitio web. Para simplificar, se recomienda utilizar Google Chrome (https://chrome.google.com/) y la extensi√≥n Selector Gadget para este buscador.^[Se puede instalar desde este enlace https://chrome.google.com/webstore/detail/selectorgadget/mhjhnkcfbdhnjickkkdbjoemdmbfginb.] La extensi√≥n Selector Gadget tambi√©n funciona en Firefox y en Safari (as√≠ como en otros navegadores), s√≥lo tienes que arrastrar el marcador a tu barra de marcadores.

Primero cargamos la p√°gina como un objeto para que luego podamos leerla como un archivo html. En este caso queremos que los t√≠tulos de los comunicados de prensa de la Organizaci√≥n de Estados Americanos (OEA) para octubre de 2019

```r
download_html(url  = "https://www.oas.org/es/centro_noticias/
              comunicados_prensa.asp?nMes=10&nAnio=2019", 
              file = "webs/comunicados_oea_10_2019.html")
```

### Caraga el html para trabajar con R

Una vez que descargamos los datos, los importamos en R usando `read_html()`

```r
web_comunicados_10_2019 <- read_html("webs/comunicados_oea_10_2019.html", 
                                     encoding = "UTF-8")
```


### Extraer la informaci√≥n con `html_nodes()` + `html_text()`

A continuaci√≥n, abrimos la p√°gina en Google Chrome donde lo primero que veremos es el sitio con las noticias. All√≠ hacemos clic en la extensi√≥n Chrome del Selector Gadget  como se indica en la figura de abajo (es la peque√±a lupa en la esquina superior derecha).

<div class="figure" style="text-align: center">
<img src="00-images/web-mining/web-mining1.png" alt="Pantallazo del sitio de los comunicados de prensa de la OEA" width="100%" />
<p class="caption">(\#fig:unnamed-chunk-8)Pantallazo del sitio de los comunicados de prensa de la OEA</p>
</div>

El primer paso es encontrar el selector de CSS que contiene nuestra informaci√≥n. En la mayor√≠a de los casos s√≥lo usaremos [SelectorGadget](https://selectorgadget.com/). Empecemos con los t√≠tulos. Como podemos ver en la figura, seleccionamos la parte del sitio web que queremos extraer. En este caso queremos los t√≠tulos de los comunicados de prensa que despu√©s de ser seleccionados se destacan en amarillo. Observar√°n que al hacer clic en uno de ellos, aparece un mensaje ".itemmenulink" en el espacio vac√≠o que hay en el Selector. Estos son los caracteres designados para los t√≠tulos en este sitio web

<div class="figure" style="text-align: center">
<img src="00-images/web-mining/web-mining2.png" alt="T√≠tulos de los comunicados de prensa que queremos extraer" width="100%" />
<p class="caption">(\#fig:pressure)T√≠tulos de los comunicados de prensa que queremos extraer</p>
</div>

A continuaci√≥n creamos un objeto para leer esta informaci√≥n como un objeto html en R. Lo llamaremos `titulos_web_comunicados_10_2019`. Dentro de la funci√≥n `html_nodes()` a√±adimos el car√°cter ".itemmenulink" ya que representa los t√≠tulos seg√∫n el selector.


```r
titulos_web_comunicados_10_2019 <- web_comunicados_10_2019 %>% 
  html_nodes(".itemmenulink") %>% 
  html_text()
```

As√≠ es como obtuvimos los titulares de todo el sitio web. 

Ahora revisaremos los primeros 10 resultados para ver c√≥mo se clasificaron los datos. La idea m√°s adelante ser√≠a transformar este objeto en un marco de datos. Tambi√©n podemos extraer el contenido de cada noticia, pero para ello necesitar√≠amos la URL en ingl√©s (o en el idioma que prefieras) de cada comunicado de prensa.


```r
head(titulos_web_comunicados_10_2019, n = 10)
##  [1] "Ministros de Seguridad de las Am√É¬©ricas adoptan recomendaciones de Quito para fortalecer la prevenci√É¬≥n y lucha contra la delincuencia organizada"                  
##  [2] "La Secretar√É¬≠a General de la OEA inicia hoy el an√É¬°lisis de integridad electoral y auditor√É¬≠a del c√É¬≥mputo oficial en Bolivia"                                      
##  [3] "El Salvador ser√É¬° sede de la VIII Reuni√É¬≥n de Ministros de Seguridad P√É¬∫blica de la OEA en 2021"                                                                    
##  [4] "Pronunciamiento de los Ministros en Materia de Seguridad P√É¬∫blica de las Am√É¬©ricas"                                                                                 
##  [5] "Recomendaciones de Quito para el Fortalecimiento de la Cooperaci√É¬≥n Internacional en Materia de Seguridad P√É¬∫blica en la Prevenci√É¬≥n y Lucha Contra la Delincuencia"
##  [6] "Secretario General de la OEA rechaza masacre de ind√É¬≠genas en Cauca, Colombia"                                                                                      
##  [7] "√Ç¬øQu√É¬© es la Reuni√É¬≥n de Ministros en Materia de Seguridad P√É¬∫blica de las Am√É¬©ricas (MISPA)?"                                                                      
##  [8] "Informe Preliminar de la Misi√É¬≥n de Veedur√É¬≠a Electoral de la OEA para las Elecciones de Autoridades Territoriales en Colombia"                                     
##  [9] "OEA recibe contribuci√É¬≥n de Estados Unidos al Programa de Capacitaci√É¬≥n sobre Delito Cibern√É¬©tico"                                                                  
## [10] "Consejo Permanente de la OEA celebra D√É¬≠a de la Visibilidad Intersexual y recibe Informe de la Ombudsperson"
```

Para eliminar los enlaces de un elemento, en lugar de su texto, debemos sustituir `html_text()`por `html_attr("href")`:


```r
links_web_comunicados_10_2019 <- web_comunicados_10_2019 %>% 
  html_nodes(".itemmenulink") %>% 
  html_attr("href") %>%
  str_c("https://www.oas.org/es/centro_noticias/",.)
links_web_comunicados_10_2019
##  [1] "https://www.oas.org/es/centro_noticias/comunicado_prensa.asp?sCodigo=C-092/19"  
##  [2] "https://www.oas.org/es/centro_noticias/comunicado_prensa.asp?sCodigo=C-091/19"  
##  [3] "https://www.oas.org/es/centro_noticias/fotonoticia.asp?sCodigo=FNC-97905"       
##  [4] "https://www.oas.org/es/centro_noticias/comunicado_prensa.asp?sCodigo=D-023/19"  
##  [5] "https://www.oas.org/es/centro_noticias/comunicado_prensa.asp?sCodigo=D-022/19"  
##  [6] "https://www.oas.org/es/centro_noticias/comunicado_prensa.asp?sCodigo=C-090/19"  
##  [7] "https://www.oas.org/es/centro_noticias/comunicado_prensa.asp?sCodigo=D-021/19"  
##  [8] "https://www.oas.org/es/centro_noticias/comunicado_prensa.asp?sCodigo=D-020/19"  
##  [9] "https://www.oas.org/es/centro_noticias/comunicado_prensa.asp?sCodigo=C-089/19"  
## [10] "https://www.oas.org/es/centro_noticias/comunicado_prensa.asp?sCodigo=AVI-209/19"
## [11] "https://www.oas.org/es/centro_noticias/comunicado_prensa.asp?sCodigo=AVI-206/19"
## [12] "https://www.oas.org/es/centro_noticias/comunicado_prensa.asp?sCodigo=C-087/19"  
## [13] "https://www.oas.org/es/centro_noticias/fotonoticia.asp?sCodigo=FNC-97847"       
## [14] "https://www.oas.org/es/centro_noticias/comunicado_prensa.asp?sCodigo=D-019/19"  
## [15] "https://www.oas.org/es/centro_noticias/comunicado_prensa.asp?sCodigo=AVI-203/19"
## [16] "https://www.oas.org/es/centro_noticias/comunicado_prensa.asp?sCodigo=C-086/19"  
## [17] "https://www.oas.org/es/centro_noticias/comunicado_prensa.asp?sCodigo=AVI-200/19"
## [18] "https://www.oas.org/es/centro_noticias/comunicado_prensa.asp?sCodigo=C-085/19"  
## [19] "https://www.oas.org/es/centro_noticias/comunicado_prensa.asp?sCodigo=C-083/19"  
## [20] "https://www.oas.org/es/centro_noticias/comunicado_prensa.asp?sCodigo=C-082/19"  
##  [ reached getOption("max.print") -- omitted 19 entries ]
```

Ahora podemos crear un *marco de datos* (data frame) con toda la informaci√≥n:

```r
df_web_comunicados_10_2019 <- tibble(
  titulo = titulos_web_comunicados_10_2019,
  link   = links_web_comunicados_10_2019)

df_web_comunicados_10_2019
## # A tibble: 39 x 2
##   titulo                               link                              
##   <chr>                                <chr>                             
## 1 Ministros de Seguridad de las Am√É¬©r‚Ä¶ https://www.oas.org/es/centro_not‚Ä¶
## 2 La Secretar√É¬≠a General de la OEA in‚Ä¶  https://www.oas.org/es/centro_not‚Ä¶
## 3 El Salvador ser√É¬° sede de la VIII R‚Ä¶ https://www.oas.org/es/centro_not‚Ä¶
## # ‚Ä¶ with 36 more rows
```

> **Ejercicio 12A.** Consigue la fecha de cada comunicado de prensa de la OEA para octubre de 2019. Llama al vector "web_date_releases_10_2019". 
>
> **Ejercicio 12B.** Consigue los t√≠tulos de las noticias de la p√°gina web de la revista *The Economist* como su secci√≥n internacional: https://www.economist.com/international/?page=1



### Iteraciones

Las iteraciones nos permiten repetir la misma operaci√≥n para un conjunto de elementos. Analicemos la sintaxis en el siguiente ejemplo:


```r
print(10)
```

[1] 10


```r
walk(.x = 1:10,
     .f = ~ {
       print(.x)
     })
```

[1] 1
[1] 2
[1] 3
[1] 4
[1] 5
[1] 6
[1] 7
[1] 8
[1] 9
[1] 10

Podr√≠amos descargar varias p√°ginas del sitio web de la OEA con una iteraci√≥n. Por ejemplo, descarguemos los comunicados de prensa de todos los meses de 2016 a 2018. Para ello, utilizamos la funci√≥n `cross_df()` para crear un *data.frame* que contiene todas las combinaciones posibles entre meses y a√±os:


```r
iteration_df <- cross_df(list(month = 1:12,year = 2016:2018))
```

Entonces, usamos la funci√≥n `walk2()` que recibir√° 2 argumentos que se iterar√°n, los meses y los a√±os.


```r
walk2(.x = iteration_df$month,
      .y = iteration_df$year,
      .f = ~ {
        Sys.sleep(2) # stops
        download_html(
          url  = glue("https://www.oas.org/es/centro_noticias/
                      comunicados_prensa.asp?nMes={.x}&nAnio={.y}"),
          file = glue("webs/comunicados_oea_{.x}_{.y}.html"))
      })
```

Nuestros pr√≥ximos pasos te mostrar√°n c√≥mo procesar estos sitios web (en este caso haremos 37 URL, pero podr√≠an ser muchos m√°s) para crear una base de datos √∫nico. Utilizaremos funciones e iteraciones personalizadas en el proceso.


### Funciones personalizadas (recetas)

Lo que hicimos hasta ahora para obtener nuestro *marco de datos* con informaci√≥n del sitio web de la OEA puede resumirse en los siguientes pasos:

1. Descargar el sitio 

2. Sube el html a la 'R'

3. Extraer los vectores para el t√≠tulo y el enlace usando selectores

4. Crear el *marco de datos* con estos vectores

Esto puede ser pensado como una "receta", que deber√≠a funcionar para cualquier archivo. Por lo tanto, lo crearemos como una funci√≥n personalizada para que trabajes con ella:

```r
f_procesar_sitio <- function(file){
  web <- read_html(file, encoding = "UTF-8")
  
  titulos <- web %>% 
    html_nodes(".itemmenulink") %>% 
    html_text()
  
  links <- web %>% 
  html_nodes(".itemmenulink") %>% 
  html_attr("href") %>%
  str_c("https://www.oas.org/es/centro_noticias/",.)
  
  df_info <- tibble(
    titulo = titulos,
    link   = links
  )
  
  return(df_info) # what returns/delivers the function
}
```

Ahora la funci√≥n toma cualquier archivo como argumento y nos da lo que esperamos:


```r
f_procesar_sitio(file = "webs/comunicados_oea_10_2016.html")
## # A tibble: 24 x 2
##   titulo                               link                              
##   <chr>                                <chr>                             
## 1 "Asamblea General Extraordinaria de‚Ä¶ https://www.oas.org/es/centro_not‚Ä¶
## 2 "ATENCI√É\u0093N - ACTUALIZA FECHA Y‚Ä¶ https://www.oas.org/es/centro_not‚Ä¶
## 3 "La OEA y la UNESCO buscan reducir ‚Ä¶ https://www.oas.org/es/centro_not‚Ä¶
## # ‚Ä¶ with 21 more rows
```

As√≠, podemos iterar esta funci√≥n en nuestros 37 archivos para crear una base de datos completo. En este caso no usaremos `walk()`, sino `map_dfr()` --esta funci√≥n espera que cada iteraci√≥n devuelva un *marco de datos*, y (debajo) los pega en orden con `bind_rows()`.

```r
archivos <- list.files("webs/", full.names = T)
archivos
##  [1] "webs//comunicados_oea_1_2016.html" 
##  [2] "webs//comunicados_oea_1_2017.html" 
##  [3] "webs//comunicados_oea_1_2018.html" 
##  [4] "webs//comunicados_oea_10_2016.html"
##  [5] "webs//comunicados_oea_10_2017.html"
##  [6] "webs//comunicados_oea_10_2018.html"
##  [7] "webs//comunicados_oea_10_2019.html"
##  [8] "webs//comunicados_oea_11_2016.html"
##  [9] "webs//comunicados_oea_11_2017.html"
## [10] "webs//comunicados_oea_11_2018.html"
## [11] "webs//comunicados_oea_12_2016.html"
## [12] "webs//comunicados_oea_12_2017.html"
## [13] "webs//comunicados_oea_12_2018.html"
## [14] "webs//comunicados_oea_2_2016.html" 
## [15] "webs//comunicados_oea_2_2017.html" 
## [16] "webs//comunicados_oea_2_2018.html" 
## [17] "webs//comunicados_oea_3_2016.html" 
## [18] "webs//comunicados_oea_3_2017.html" 
## [19] "webs//comunicados_oea_3_2018.html" 
## [20] "webs//comunicados_oea_4_2016.html" 
##  [ reached getOption("max.print") -- omitted 53 entries ]

df_web_comunicados_2016_2018 <- map_dfr(.x = archivos,
                                 .f = ~ {
                                   f_procesar_sitio(.x)
                                 })
df_web_comunicados_2016_2018
## # A tibble: 1,397 x 2
##   titulo                               link                              
##   <chr>                                <chr>                             
## 1 "Misi√É¬≥n Especial de la OEA llegar√É‚Ä¶ https://www.oas.org/es/centro_not‚Ä¶
## 2 "OEA enviar√É¬° Misi√É¬≥n Especial a Ha‚Ä¶ https://www.oas.org/es/centro_not‚Ä¶
## 3 "ATENCI√É\u0093N CAMBIO DE HORA: Con‚Ä¶ https://www.oas.org/es/centro_not‚Ä¶
## # ‚Ä¶ with 1,394 more rows
```

## Usando APIs y extrayendo datos de Twitter 

Twitter es una red social fundada en 2006 que permite a los usuarios interactuar entre s√≠ enviando mensajes de no m√°s de 280 caracteres. Es ampliamente usada por servicios p√∫blicos y por pol√≠ticos especialmente en las √©pocas de campa√±a electoral. Con las nuevas t√©cnicas de an√°lisis de datos el estudio de la interacci√≥n de los usuarios en twitter se ha vuelto muy importante por ejemplo para medir los temas sobre los que est√° hablando la gente (*trending topics*) y sobre si las opiniones sobre una persona o tema son positivas o negativas. Por ejemplo Ernesto Calvo en su libro *Anatom√≠a Pol√≠tica de Twitter en Argentina: Tuiteando #NISMAN* en el que identifica a trav√©s del estudio de los tweets sobre las causas de la muerte del ex fiscal Alberto Nisman que reflejaban (y estaban muy correlacionados con) las divisiones pol√≠ticas de la oposici√≥n del gobiernon. A partir del caso Nisman, Ernesto Calvo analiza los tweets de los usuarios argentinos y muestra que la polarizaci√≥n mezcla pol√≠tica, algoritmos y smartphones.

Si has le√≠do la secci√≥n anterior, ya conoces los paquetes R que se utilizar√°n en esta secci√≥n del libro. Para hacer la extracci√≥n de datos de Twitter la mejor opci√≥n en mano es `Rtweet`, que permite acceder gratuitamente al API de Twitter para descargar informaci√≥n de los usuarios, temas de tendencias y hashtags. Para extraer datos de Twitter con R se recomienda consultar *Twitter as Data*, que contiene algunas rutinas estandarizadas para descargar datos de esta plataforma.

##Algunos antecedentes en APIs

La interfaz de programaci√≥n de aplicaciones *Application Program Interfaces* (APIs en ingl√©s) son un set de protocolos y funciones que gobiernan ciertas interacciones entre aplicaciones web y usuarios.

las APIs son similares a los navegadores web pero cumplen diferentes prop√≥sitos:

    -Navegadores web reproducen contenido de los browsers 
    -Las APIs permiten manipular y organizador datos

Para que las APIs p√∫blicas sean utilizadas muchos sitios solo permiten a usuarios autorizados (por ejemplo aquellos que tienen una cuenta en la plataforma). Este es el caso para Twitter, Facebook, Instagram and Github.
    

Si bien estas APIs son ampliamente conocidas no est√° dem√°s mencionar algunas creadas por la misma comunidad de R especializada en datos pol√≠ticos. Por ejemplo el paquete `lobbyR` creado por Daniel Alcatruz ^[https://github.com/Dalcatruz/lobbyR] que permite cargar y estructurar datos desde la API lobby que se encuentra en la plataforma de Ley de Lobby (https://www.leylobby.gob.cl/) implementada para el Gobierno de Chile, que permite realizar consultas por ejemplo sobre audiencias en determinados servicios p√∫blicos y organismos del estado como el congreso y los municipios. Otro paquete que es necesario mencionar es `inegiR` creado por Eduardo Flores que permite interactuar con la API del INEGI (Instituto Nacional de Estad√≠stica y Geograf√≠a de M√©xico) para realizar consultas espec√≠ficas. http://enelmargen.org/ds/inegiR/vignette_spa.html


#Extraer los datos

Lo primero es no depender de la API oficial de Twitter, y por lo tanto deber√≠as tener tu propia cuenta de Twitter que puedes crear en (https://twitter.com/i/flow/signup). 
Luego procedes a cargar el paquete `rtweet` que te permitir√° extraer datos de Twitter. Te mostraremos esta rutina y extraeremos datos de diferentes usuarios y hashtags.


```r
library(rtweet)
```

Por ejemplo podemos obtener los IDs de las cuentas que sigue la ONU en espa√±ol. Por defecto la funci√≥n te muestra hasta 5000 usuarios.


```r
## Obtener los ID que son seguidos por la cuenta de la ONU en espa√±ol
friends_ONU_es <- get_friends("ONU_es")
```

Para saber m√°s informaci√≥n de estos usuarios utilizamos la `lookup_users()` 


```r
info_friends_ONU_es <- lookup_users(friends_ONU_es$user_id)
```

Podemos obtener informaci√≥n de los seguidores tambi√©n mediante la funci√≥n `get_followers()`. Como la cuenta de twitter de ONU en espa√±ol tiene m√°s de un mill√≥n de seguidores, es m√°s comodo hacer un an√°isis de una porci√≥n de ellos. Para obtener todos los IDs de usuarios que siguen a \@ONU_es, solo necesitas 2 cosas:

- Una conexi√≥n estable a Internet
- Tiempo - aproximadamente cinco d√≠as y medio


```r
followers_ONU_es <- get_followers("ONU_es", n = 200, 
                                  retryonratelimit = TRUE)
```

Aqu√≠ obtenemos la informaci√≥n de los usuarios que siguen la cuenta \@ONU_es


```r
info_followers_ONU_es<-lookup_users(followers_ONU_es$user_id)
```


#### Buscando tweets especificos

Ahora estamos listos para buscar tweets recientes. Busquemos por ejemplo todos los tweets que lleven el hashtag "#Pi√±era", en alusi√≥n al presidente chileno. Recordemos que un hashtag es una etiqueta que corresponde a una palabra antecedida por el s√≠mbolo "#". Esta es utilizada principalmente para encontrar tweets con contenido similar, ya que el fin es que tanto el sistema como el usuario la identifiquen de forma r√°pida. Utilizaremos la funci√≥n rtweet::search_tweets(). Esta funci√≥n requiere los siguientes argumentos

    `q`: la palabra que quieres buscar
    `n`: el n√∫mero de tweets m√°ximos que quieres extraer. Puedes pedir un m√°ximo de 18.000 tweets cada 15 minutos.

> Tip: Para ver otros argumentos de la funci√≥n puedes usar el documento de ayuda
>

```r
?search_tweets
```

Un aspecto importante que tenemos que aclarar con respecto a las consultas que le hacemos a la API de Twitter es que los resultados obtenidos no podr√°n reproducir completamente, estos cambiar√°n, ya que la versi√≥n gratuita de Twitter nos permite hacer consultas en un periodo terminado de tiempo en el pasado (por ejemplo una semana) por lo que los resultados de las consultas cambiar√°n con el tiempo. Si te interesa una cierta agenda o hashtag, un buen consejo es descargarla regularmente y guardarla en un disco duro. No podr√°s volver a ellas en el futuro.


```r
# Look for 500 tweets with the hashtag #pi√±era
pinera_tweets <- search_tweets(q = "#pi√±era",
                               n = 1000)
# We see the first 3 columns
head(pinera_tweets, n = 3)
## # A tibble: 3 x 90
##   user_id status_id created_at          screen_name text  source
##   <chr>   <chr>     <dttm>              <chr>       <chr> <chr> 
## 1 122154‚Ä¶ 12951194‚Ä¶ 2020-08-16 22:05:08 Flako823    "ü§£ c‚Ä¶ Twitt‚Ä¶
## 2 122154‚Ä¶ 12950348‚Ä¶ 2020-08-16 16:28:52 Flako823    "Lad‚Ä¶ Twitt‚Ä¶
## 3 142816‚Ä¶ 12951192‚Ä¶ 2020-08-16 22:04:19 CiudadanoC‚Ä¶ "¬øCu‚Ä¶ Twitt‚Ä¶
## # ‚Ä¶ with 84 more variables
```

Para obtener informaci√≥n de los usuarios que est√°n emitiendo tweets sobre #pi√±era en el momento que estamos escribiendo este capitulo  


```r
lookup_users(pinera_tweets$user_id)
## # A tibble: 860 x 90
##   user_id status_id created_at          screen_name text  source
##   <chr>   <chr>     <dttm>              <chr>       <chr> <chr> 
## 1 122154‚Ä¶ 12951194‚Ä¶ 2020-08-16 22:05:08 Flako823    ü§£ co‚Ä¶ Twitt‚Ä¶
## 2 142816‚Ä¶ 12951194‚Ä¶ 2020-08-16 22:05:20 CiudadanoC‚Ä¶ Cree‚Ä¶ Twitt‚Ä¶
## 3 119324‚Ä¶ 12951192‚Ä¶ 2020-08-16 22:04:14 marvelalre‚Ä¶ Se v‚Ä¶ Twitt‚Ä¶
## # ‚Ä¶ with 857 more rows, and 84 more variables
```


Las consultas de tweets pueden hacerse de varias maneras dependiendo en los que estas buscando:

### Buscar una palabra clave


```r
query_1 <- search_tweets(q = "pi√±era", n = 20)
```


### Buscar una frase


```r
query_2 <- search_tweets(q = "pi√±era economia", n = 20)
```


#### Buscar m√∫ltiples palabras claves


```r
query_3 <- search_tweets(q = "pi√±era AND bachelet", n = 20)
```

Por defecto `search_tweets()` retorna 100 tweets. Para retornar m√°s el n debe ser mayor, esto se hace con `n =`.

> **Ejercicio 12C.** En lugar de usar AND, usa OR entre los diferentes t√©rminos de b√∫squeda. Ambas b√∫squedas devolver√°n tweets muy diferentes. 

#### Busca cualquier menci√≥n de una lista de palabras


```r
query_4 <- search_tweets("bolsonaro OR pi√±era OR macri", n = 20)
```

> **Ejercicio 12D.** Buscar tweets en espa√±ol que no sean retweets

#### Especifica el lenguaje de los tweets y excluye los retweets


```r
query_5 <- search_tweets("pi√±era", lang = "es", include_rts = FALSE, n=20)
```

### Descargando caracter√≠sticas relacionadas con los usuarios de Twitter

#### Retweets

Un retweet es cuando t√∫ o alguien m√°s comparte un tweet para que tus seguidores puedan verlo. Es similar a la caracter√≠stica de "compartir" en Facebook. Hagamos la misma rutina de antes pero esta vez ignorando los retweets. Lo hacemos con el argumento `include_rts` definido como `FALSE`. Entonces podemos obtener tweets y retweets en un marco de datos separado

>Nota: La opci√≥n `include_rts =` es muy √∫til si est√°s estudiando los patrones de viralizaci√≥n de ciertos hashtags.


```r
# Look for 500 tweets with the hashtag #pi√±era, but ignore retweets
pinera_tweets <- search_tweets("#pi√±era", 
                               n = 500,
                               include_rts = FALSE)
# See the first two columns
head(pinera_tweets, n = 2)
## # A tibble: 2 x 90
##   user_id status_id created_at          screen_name text  source
##   <chr>   <chr>     <dttm>              <chr>       <chr> <chr> 
## 1 604708‚Ä¶ 12951150‚Ä¶ 2020-08-16 21:47:50 TIRSO48     El #‚Ä¶ Twitt‚Ä¶
## 2 604708‚Ä¶ 12950584‚Ä¶ 2020-08-16 18:02:49 TIRSO48     #Pi√±‚Ä¶ Twitt‚Ä¶
## # ‚Ä¶ with 84 more variables
```

Ahora veamos qui√©n est√° twiteando sobre el hashtag "#pi√±era"


```r
# Look at the column with the names - top 6
head(pinera_tweets$screen_name)
## [1] "TIRSO48"     "TIRSO48"     "TIRSO48"     "Seba_fabres" "Seba_fabres"
## [6] "nacioncl"

unique(pinera_tweets$screen_name)
##  [1] "TIRSO48"         "Seba_fabres"     "nacioncl"       
##  [4] "minevargasg"     "_hexagram_"      "CiudadannoChile"
##  [7] "rechazoviejo"    "MolinavSeba"     "marcelotobarc"  
## [10] "peval29"         "tiomattu"        "solangeeguia"   
## [13] "hugoguionista"   "PaoprimeraL"     "PabloPincel"    
## [16] "ElPobreHank"     "matera_di"       "fcojavier_cl"   
## [19] "peyoaguilera"    "go4kchile"      
##  [ reached getOption("max.print") -- omitted 339 entries ]
```

Tambi√©n podemos usar la funci√≥n `search_users()` para explorar qu√© usuarios est√°n twiteando usando un hashtag particular. Esta funci√≥n extrae un data.frame de los usuarios e informaci√≥n sobre sus cuentas.


```r
# Which users are tweeting about #pi√±era?
users <- search_users("#pi√±era",
                      n = 500)
# see the first 2 users (the data frame is large)
head(users, n = 2)
## # A tibble: 2 x 90
##   user_id status_id created_at          screen_name text  source
##   <chr>   <chr>     <dttm>              <chr>       <chr> <chr> 
## 1 201853‚Ä¶ 11916828‚Ä¶ 2019-11-05 11:45:16 Plaid_Pine‚Ä¶ Of c‚Ä¶ Twitt‚Ä¶
## 2 947001‚Ä¶ 11362572‚Ä¶ 2019-06-05 13:03:23 robbiepine‚Ä¶ @PLD‚Ä¶ Twitt‚Ä¶
## # ‚Ä¶ with 84 more variables
```

Aprendamos m√°s sobre estas personas. ¬øDe d√≥nde son? Como vemos, hay 304 lugares √∫nicos por lo que el gr√°fico que usamos para trazar la informaci√≥n no es addecuado para visualizarlo.


```r
# How many places are represented?
length(unique(users$location))
## [1] 227
## [1] 304

users %>%
  ggplot(aes(location)) +
  geom_bar() + 
  coord_flip() +
  labs(x = "Frecuencia",
       y = "Ubicaci√≥n",
       title = "Cuentas de Twitter - Ubicaciones √∫nicas")
```

<img src="12-web-mining_es_files/figure-html/unnamed-chunk-36-1.png" width="672" style="display: block; margin: auto;" />

Ordenemos por frecuencia las ubicaciones m√°s nombradas y las trazamos. Para ello utilizamos `top_n()` que extraer√° las localizaciones con al menos 20 usuarios asociados a ella


```r
users %>%
  count(location, sort = TRUE) %>%
  mutate(location = reorder(location, n)) %>%
  top_n(20) %>%
  ggplot(aes(x = location, y = n)) +
  geom_col() +
  coord_flip() +
      labs(x = "Frecuencia",
      y = "Ubicaci√≥n",
      title = "¬øDe d√≥nde son estas cuentas de Twitter? Ubicaciones √∫nicas")
```

<img src="12-web-mining_es_files/figure-html/unnamed-chunk-37-1.png" width="672" style="display: block; margin: auto;" />

Es mejor si quitamos los NAs para ver los lugares m√°s claramente. En general, la geolocalizaci√≥n en Twitter es bastante mediocre porque es voluntaria y pocas personas la tienen activada en sus tel√©fonos m√≥viles. As√≠ que lo que vemos es gente que quiere que sepamos su ubicaci√≥n, lo que significa que hay un sesgo en estos resultados.


```r
users %>%
  count(location, sort = T) %>%
  mutate(location = reorder(location,n)) %>%
  na.omit() %>% filter(location != "") %>% 
  top_n(20) %>%
  ggplot(aes(x = location,y = n)) +
  geom_col() +
  coord_flip() +
      labs(x = "Ubicaci√≥n",
      y = "Frecuencia",
      title = "Usuarios de Twitter - Ubicaciones √∫nicas")
```

<img src="12-web-mining_es_files/figure-html/unnamed-chunk-38-1.png" width="672" style="display: block; margin: auto;" />

Finalmente repetimos el ejercicio usando hashtags que se refieren a Michelle Bachelet, la ex presidenta de Chile:


```r
## Buscar tweets que usen el hashtag #bachelet
rt <- search_tweets(
  "#bachelet", n = 600, include_rts = FALSE
)
```

¬øC√≥mo ha sido la tendencia temporal de este hashtag en los √∫ltimos d√≠as?

```r
## Graficar una serie de tiempo de tweets
rt %>%
  ts_plot("3 hours") +
  ggplot2::theme_minimal() +
  ggplot2::theme(plot.title = ggplot2::element_text(face = "bold")) +
  ggplot2::labs(
    x = NULL, y = NULL,
    title = "Frecuencia de #bachelet en los tweets de los √∫ltimos 9 d√≠as",
    subtitle = "Conteo de tweets usando intervalos de tres horas",
    caption = "Fuente: Datos recolectados desde la API de Twitter, usando el paquete rtweet"
  )
```

<img src="12-web-mining_es_files/figure-html/unnamed-chunk-40-1.png" width="672" style="display: block; margin: auto;" />

Esperamos que este cap√≠tulo haya sido √∫til. En el Cap√≠tulo \@ref(qta) te mostraremos c√≥mo explorar m√°s a fondo los datos de Twitter una vez que los hayas descargado.
